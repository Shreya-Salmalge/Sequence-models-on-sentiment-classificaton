{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3228,"status":"ok","timestamp":1662470089078,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"},"user_tz":-330},"id":"kGGrncpXSV-1","outputId":"15ee77ea-8a78-452f-928e-e00f08d41a2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":[],"metadata":{"id":"IRQBwbYv8OJj"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1662470089080,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"},"user_tz":-330},"id":"x3XJ35te6PYR","outputId":"17dd39cd-7288-4573-ec04-5520f55c6a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dlnlp/assign2\n"]}],"source":["cd drive/MyDrive/dlnlp/assign2"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-alHHgb9gAgx","executionInfo":{"status":"ok","timestamp":1662470093958,"user_tz":-330,"elapsed":4890,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"}}},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import torch.utils.data as data_utils\n","\n","import string\n","import numpy as np\n","import pandas as pd\n","import re"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1668,"status":"ok","timestamp":1662470143810,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"},"user_tz":-330},"id":"Py8Py0kxZvfz","outputId":"d909d97f-cade-4b2e-dc26-1b116f4a02c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["40000\n"]}],"source":["fileName = \"Train dataset.csv\"\n","filenameTest = 'test_Dataset.csv'\n","\n","df = pd.read_csv(fileName)\n","print(len(df))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aG8N8Mu42JZA","executionInfo":{"status":"ok","timestamp":1662470100585,"user_tz":-330,"elapsed":592,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"}}},"outputs":[],"source":["# from gensim.models import Word2Vec, KeyedVectors\n","# w2vmodel = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"cgOdos5_8eQE","executionInfo":{"status":"ok","timestamp":1662470120796,"user_tz":-330,"elapsed":19572,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"}}},"outputs":[],"source":["def read_glove_vector(glove_vec):\n","  with open(glove_vec, 'r', encoding='UTF-8') as f:\n","    word_to_vec_map = {}\n","    for line in f:\n","      w_line = line.split()\n","      curr_word = w_line[0]\n","      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n","\n","  return word_to_vec_map\n","\n","gloveFile = \"glove.6B.200d.txt\"\n","word_to_vec_map = read_glove_vector(gloveFile)\n","# print(word_to_vec_map['rock'])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1662470120799,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"},"user_tz":-330},"id":"lKOZgGFWaRnU"},"outputs":[],"source":["max_words = 300\n","\n","def remove_tags(x):\n","    result = re.sub('<.*?>','',x)\n","    return result\n","\n","def to_lo_vecs(sentence):\n","  lo_words =  sentence.strip().split()\n","  if len(lo_words)<max_words:\n","    lo_words = lo_words + [0]* (max_words-len(lo_words))\n","  else:\n","    lo_words = lo_words[:max_words]\n","  lo_vecs_glove = [word_to_vec_map['unk'] if x not in word_to_vec_map.keys() else word_to_vec_map[x] for x in lo_words]\n","  # lo_vecs_w2v = [w2vmodel['unk'] if x not in w2vmodel.wv.vocab else w2vmodel[x] for x in lo_words]\n","  # print(lo_vecs_glove.shape)\n","  # lo_vecs = np.append([lo_vecs_glove],[lo_vecs_w2v], axis = 1)\n","  return lo_vecs_glove #lo_vecs\n"]},{"cell_type":"code","source":["\n","# Preprocessing\n","# df.loc[:,\"review\"] = df.review.apply(lambda x : str.lower(x))\n","# df['review'] = df.review.apply(lambda x : remove_tags(x))\n","# df.loc[:,\"review\"] = df.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n","# df.loc[:,\"review\"] = df.review.apply(lambda sentence : to_lo_vecs(sentence))\n","\n","df_temp = df[df.sentiment == 'positive']\n","df_temp.drop('sentiment', inplace=True, axis=1)\n","\n","lenPos = len(df_temp)\n","print(lenPos)\n","\n","# Preprocessing\n","df_temp.loc[:,\"review\"] = df_temp.review.apply(lambda x : str.lower(x))\n","df_temp['review'] = df_temp.review.apply(lambda x : remove_tags(x))\n","df_temp.loc[:,\"review\"] = df_temp.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n","df_temp.loc[:,\"review\"] = df_temp.review.apply(lambda sentence : to_lo_vecs(sentence))\n","\n","vecDataset = df_temp['review'].tolist()\n","del df_temp\n","x_data = torch.tensor(vecDataset, dtype=torch.float)\n","torch.save(x_data,\"x_data_p.pt\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nl-LUWTP1nL1","executionInfo":{"status":"ok","timestamp":1662469274316,"user_tz":-330,"elapsed":114897,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"}},"outputId":"01d415a0-6f28-421c-d8ed-20e230f5859c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self.obj[selected_item_labels] = value\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  from ipykernel import kernelapp as app\n"]},{"output_type":"stream","name":"stdout","text":["19885\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"]}]},{"cell_type":"code","source":["df_temp = df[df.sentiment == 'negative'] # df.loc[df.sentiment == 'positive', 'sentiment'] = 1\n","df_temp.drop('sentiment', inplace=True, axis=1)\n","# Preprocessing\n","df_temp.loc[:,\"review\"] = df_temp.review.apply(lambda x : str.lower(x))\n","df_temp['review'] = df_temp.review.apply(lambda x : remove_tags(x))\n","df_temp.loc[:,\"review\"] = df_temp.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n","df_temp.loc[:,\"review\"] = df_temp.review.apply(lambda sentence : to_lo_vecs(sentence))\n","\n","lenNeg = len(df_temp)\n","print(lenNeg)\n","\n","vecDataset = df_temp['review'].tolist()\n","del df_temp\n","x_data = torch.tensor(vecDataset, dtype=torch.float)\n","torch.save(x_data,\"x_data_n.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLrv_XC63AAY","executionInfo":{"status":"ok","timestamp":1662470296290,"user_tz":-330,"elapsed":138367,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"}},"outputId":"97b296bb-ae78-4cf3-a1fe-6c406e7d7d2a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self.obj[selected_item_labels] = value\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n"]},{"output_type":"stream","name":"stdout","text":["20115\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  \n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1662465476218,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"},"user_tz":-330},"id":"GYiqXrqyYE0r","outputId":"e3322c60-22e9-4c89-e9d9-a3892a2b99da"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([40000, 1])\n"]}],"source":["shape1 = (lenPos,1)\n","y1 = torch.ones(shape1)\n","shape0 = (lenNeg,1)\n","y0 = torch.zeros(shape0)\n","y = torch.cat((y1,y0))\n","print(y.size())"]},{"cell_type":"code","source":["y.save('y.pt')"],"metadata":{"id":"m0l9ajkbrXrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9tk17V_E3gL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6080,"status":"ok","timestamp":1662465482272,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"},"user_tz":-330},"id":"bYOhJ-389IJr","outputId":"5770f4bb-6457-45f9-cffd-172d643ad99b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","x_data, y = x_data.to(device), y.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oeefHn4ZFGJ"},"outputs":[],"source":["full_dataset = data_utils.TensorDataset(x_data, y)\n","train_size = int(0.80 * len(full_dataset))\n","validation_size = len(full_dataset) - train_size\n","train_dataset, validation_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n","train_loader = data_utils.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","validation_loader = data_utils.DataLoader(validation_dataset, batch_size=64, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYd1wWme0a3G"},"outputs":[],"source":["embed_len = 200\n","hidden_dim = 200\n","n_layers = 2\n","fc_dim = 100\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n","        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, dropout = 0.5)\n","        self.linear = nn.Linear(hidden_dim, fc_dim)\n","        self.linear2 = nn.Linear(fc_dim,1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, X_batch):\n","        # embeddings = self.embedding_layer(X_batch)\n","        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim).to(device), torch.randn(n_layers, len(X_batch), hidden_dim).to(device)\n","        output, (hidden, carry) = self.lstm(X_batch, (hidden, carry))\n","        y1 = torch.relu_(self.linear(output[:,-1]))\n","        y2 = self.linear2(y1)\n","        return self.sigmoid(y2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1H857f9mABhs"},"outputs":[],"source":["model = Net().to(device)"]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-rxjheBSekrj","executionInfo":{"status":"ok","timestamp":1662465492904,"user_tz":-330,"elapsed":49,"user":{"displayName":"Shreya Salmalge","userId":"09554849048150219633"}},"outputId":"21aa85e9-97a6-4805-c57a-9dc5e5733515"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (lstm): LSTM(200, 200, num_layers=2, batch_first=True, dropout=0.5)\n","  (linear): Linear(in_features=200, out_features=100, bias=True)\n","  (linear2): Linear(in_features=100, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDhMHsElAG0W"},"outputs":[],"source":["def validation_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    validation_loss, correct, incorrect = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            validation_loss += loss_fn(pred, y).item()\n","            correct += ((pred>=0.5) == y).type(torch.float).sum().item()\n","            incorrect += ((pred>=0.5) != y).type(torch.float).sum().item()\n","\n","    validation_loss /= num_batches\n","    correct /= size\n","    incorrect /= size\n","    print(f\"validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n","    print(f\"Incorrect percentage : \\n Accuracy: {(100*incorrect):>0.1f}%\\n\")\n","    return validation_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSE3xBJOZzge"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","\n","        # Compute prediction and loss\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a_NTvvjbVmE"},"outputs":[],"source":["def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    validation_loss, correct, incorrect = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            validation_loss += loss_fn(pred, y).item()\n","            correct += ((pred>=0.5) == y).type(torch.float).sum().item()\n","            incorrect += ((pred>=0.5) != y).type(torch.float).sum().item()\n","\n","    validation_loss /= num_batches\n","    correct /= size\n","    incorrect /= size\n","    print(f\"Test Stats: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n","    print(f\"Incorrect percentage : \\n Accuracy: {(100*incorrect):>0.1f}%\\n\")"]},{"cell_type":"code","source":["def getTrainLoseAndError(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    train_loss, correct, incorrect = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            train_loss += loss_fn(pred, y).item()\n","            correct += ((pred>=0.5) == y).type(torch.float).sum().item()\n","            incorrect += ((pred>=0.5) != y).type(torch.float).sum().item()\n","\n","    train_loss /= num_batches\n","    correct /= size\n","    incorrect /= size\n","    print(f\"Train Error : \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","    print(f\"Incorrect percentage : \\n Accuracy: {(100*incorrect):>0.1f}%\\n\")"],"metadata":{"id":"646fvqqSxMvm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7S8lj-1Wa41v","outputId":"6368810e-cdd9-4e58-9dc8-f6e72977edc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["In the begining :\n","Train Error : \n"," Accuracy: 49.6%, Avg loss: 0.693405 \n","\n","Incorrect percentage : \n"," Accuracy: 50.4%\n","\n","validation Error: \n"," Accuracy: 50.0%, Avg loss: 0.693261 \n","\n","Incorrect percentage : \n"," Accuracy: 50.0%\n","\n","------------------\n","Epoch 1\n","-------------------------------\n","loss: 0.691474  [    0/32000]\n","loss: 0.691481  [ 6400/32000]\n","loss: 0.695782  [12800/32000]\n","loss: 0.697862  [19200/32000]\n","loss: 0.694577  [25600/32000]\n","validation Error: \n"," Accuracy: 53.5%, Avg loss: 0.692009 \n","\n","Incorrect percentage : \n"," Accuracy: 46.5%\n","\n","Epoch 2\n","-------------------------------\n","loss: 0.690113  [    0/32000]\n","loss: 0.684309  [ 6400/32000]\n","loss: 0.692627  [12800/32000]\n","loss: 0.678823  [19200/32000]\n","loss: 0.698248  [25600/32000]\n","validation Error: \n"," Accuracy: 50.0%, Avg loss: 0.693666 \n","\n","Incorrect percentage : \n"," Accuracy: 50.0%\n","\n","Epoch 3\n","-------------------------------\n","loss: 0.692126  [    0/32000]\n","loss: 0.693184  [ 6400/32000]\n","loss: 0.695342  [12800/32000]\n","loss: 0.693434  [19200/32000]\n","loss: 0.690659  [25600/32000]\n","validation Error: \n"," Accuracy: 50.0%, Avg loss: 0.693091 \n","\n","Incorrect percentage : \n"," Accuracy: 50.0%\n","\n","Epoch 4\n","-------------------------------\n","loss: 0.694098  [    0/32000]\n","loss: 0.690859  [ 6400/32000]\n","loss: 0.692514  [12800/32000]\n","loss: 0.687949  [19200/32000]\n","loss: 0.689357  [25600/32000]\n","validation Error: \n"," Accuracy: 54.6%, Avg loss: 0.681161 \n","\n","Incorrect percentage : \n"," Accuracy: 45.4%\n","\n","Epoch 5\n","-------------------------------\n","loss: 0.676225  [    0/32000]\n","loss: 0.637842  [ 6400/32000]\n","loss: 0.690807  [12800/32000]\n","loss: 0.693411  [19200/32000]\n"]}],"source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 60\n","loss_fn = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #try adagrad too\n","print(\"In the begining :\")\n","getTrainLoseAndError(train_loader, model, loss_fn)\n","validation_loop(validation_loader, model, loss_fn)\n","print(\"------------------\")\n","vLossMin = 100\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_loader, model, loss_fn, optimizer)\n","    vLoss = validation_loop(validation_loader, model, loss_fn)\n","    if(vLossMin>vLoss):\n","      torch.save(model, \"a2_lstm_fc_do.p\")\n","      vLossMin = vLoss\n","\n","print(\"Done!\")\n","\n","print(\"In the End :\")\n","getTrainLoseAndError(train_loader, model, loss_fn)\n","validation_loop(validation_loader, model, loss_fn)\n","print(\"------------------\")\n","\n","# learning_rate = 1e-3\n","# batch_size = 64\n","# epochs = 100\n","# loss_fn = nn.BCELoss()\n","# optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate) #try adagrad too\n","\n","# for t in range(epochs):\n","#     print(f\"Epoch {t+1}\\n-------------------------------\")\n","#     train_loop(train_loader, model, loss_fn, optimizer)\n","#     test_loop(test_loader, model, loss_fn)\n","# print(\"Done!\")"]},{"cell_type":"code","source":["df_test = pd.read_csv(fileName)\n","print(len(df))\n","\n","# Preprocessing\n","df_test.loc[:,\"review\"] = df_test.review.apply(lambda x : str.lower(x))\n","df_test['review'] = df_test.review.apply(lambda x : remove_tags(x))\n","df_test.loc[:,\"review\"] = df_test.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n","### check if you want to remove stopwords\n","df_test.loc[:,\"review\"] = df_test.review.apply(lambda sentence : to_lo_vecs(sentence))\n","\n","df_testp = df[df.sentiment == 'positive']\n","df_testn = df[df.sentiment == 'negative'] # df.loc[df.sentiment == 'positive', 'sentiment'] = 1\n","\n","df_testp.drop('sentiment', inplace=True, axis=1)\n","df_testn.drop('sentiment', inplace=True, axis=1)\n","lenPosT = len(df_testp)\n","print(lenPosT)\n","lenNegT = len(df_testn)\n","print(lenNegT)\n","\n","shape1 = (lenPosT,1)\n","y1t = torch.ones(shape1)\n","shape0 = (lenNegT,1)\n","y0t = torch.zeros(shape0)\n","yt = torch.cat((y1t,y0t))\n","print(yt.size())\n","\n","vecDataset = dfp['review'].tolist()\n","vecDatasetN = dfn['review'].tolist()\n","vecDataset.extend(vecDatasetN)\n","del df_test\n","del df_testn\n","del df_testp\n","\n","testDataset = data_utils.TensorDataset(x_data_test, yt)\n","test_loader = data_utils.DataLoader(testDataset, batch_size=64, shuffle=True)"],"metadata":{"id":"mp-rwXEjoUlg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vg7wS0SRc0sA"},"outputs":[],"source":["model = torch.load(\"a2_lstm_fc_do.p\")\n","test_loop(test_loader, model, loss_fn)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1-KSceUsPtdwMcHrdYtx8tLwisWgAQRXG","timestamp":1662465595611},{"file_id":"1k5PP3jc0UK3QTUsgy_DEwRBUCCux9UFt","timestamp":1662453105321},{"file_id":"1QHedTmbmNuttOkY_npspsnYIr_nQyK4s","timestamp":1662380107221},{"file_id":"1dMuS_S7Xu5JQKNuhhnvwsCKoHbDDVSl0","timestamp":1661839676906}],"mount_file_id":"1dMuS_S7Xu5JQKNuhhnvwsCKoHbDDVSl0","authorship_tag":"ABX9TyMCgP3uQuJyY89lizpDhpXf"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}