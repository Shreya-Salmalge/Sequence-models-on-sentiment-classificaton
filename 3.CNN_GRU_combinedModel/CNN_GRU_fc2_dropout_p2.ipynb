{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGGrncpXSV-1",
        "outputId": "3e416a57-9d27-4a5f-f6b1-bcba3c92d930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3XJ35te6PYR",
        "outputId": "596d5c95-21ab-4121-dafc-00aade0942ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dlnlp/assign2\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/dlnlp/assign2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-alHHgb9gAgx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKOZgGFWaRnU"
      },
      "outputs": [],
      "source": [
        "max_words = 300\n",
        "\n",
        "def remove_tags(x):\n",
        "    result = re.sub('<.*?>','',x)\n",
        "    return result\n",
        "\n",
        "def to_lo_vecs(sentence):\n",
        "  lo_words =  sentence.strip().split()\n",
        "  if len(lo_words)<max_words:\n",
        "    lo_words = lo_words + [0]* (max_words-len(lo_words))\n",
        "  else:\n",
        "    lo_words = lo_words[:max_words]\n",
        "  lo_vecs_glove = [word_to_vec_map['unk'] if x not in word_to_vec_map.keys() else word_to_vec_map[x] for x in lo_words]\n",
        "  # lo_vecs_w2v = [w2vmodel['unk'] if x not in w2vmodel.wv.vocab else w2vmodel[x] for x in lo_words]\n",
        "  # print(lo_vecs_glove.shape)\n",
        "  # lo_vecs = np.append([lo_vecs_glove],[lo_vecs_w2v], axis = 1)\n",
        "  return lo_vecs_glove #lo_vecs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYOhJ-389IJr",
        "outputId": "4d2d2c97-a3d0-4402-d6e1-d2415c867fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYd1wWme0a3G"
      },
      "outputs": [],
      "source": [
        "embed_len = 200\n",
        "hidden_dim = 100\n",
        "n_layers = 1\n",
        "fc_dim = 100\n",
        "fc_dim2 = 50\n",
        "in_channels = 1\n",
        "out_channels = 200\n",
        "kernel_size = (3, 200)\n",
        "kernel_size_pool = (10,1)\n",
        "dropout = 0.5\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
        "        self.maxp = nn.MaxPool2d(kernel_size_pool, stride = 2)\n",
        "        self.gru = nn.GRU(input_size=out_channels, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, dropout = 0.5)\n",
        "        self.linear = nn.Linear(hidden_dim, fc_dim)\n",
        "        self.linear1 = nn.Linear(fc_dim,fc_dim2)\n",
        "        self.linear2 = nn.Linear(fc_dim2,1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # embeddings = self.embedding_layer(X_batch)\n",
        "        # print(\"X_batch : \", X_batch.shape)\n",
        "        X_batch = X_batch.unsqueeze(1) \n",
        "        # print(X_batch.shape)\n",
        "        X_batch = self.dropout(self.conv(X_batch))\n",
        "        # print(X_batch.shape)\n",
        "        X_batch = self.maxp(X_batch)\n",
        "        # print(X_batch.shape)\n",
        "        X_batch = X_batch.squeeze(dim=3)\n",
        "        # print(X_batch.shape)\n",
        "        X_batch = X_batch.swapaxes(1,2)\n",
        "        # print(X_batch.shape)\n",
        "        hidden = torch.randn(n_layers, len(X_batch), hidden_dim).to(device)\n",
        "        # print(hidden.shape)\n",
        "        output, hidden = self.gru(X_batch, hidden)\n",
        "        y1 = torch.relu_(self.dropout(self.linear(output[:,-1])))\n",
        "        y1 = torch.relu_(self.linear1(y1))\n",
        "        y2 = self.linear2(y1)\n",
        "        return self.sigmoid(y2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H857f9mABhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca71445e-2d93-436f-f54d-4215898b71cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rxjheBSekrj",
        "outputId": "48fc65a9-1c05-4eb1-82ea-2cdb3352af84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv): Conv2d(1, 200, kernel_size=(3, 200), stride=(1, 1))\n",
              "  (maxp): MaxPool2d(kernel_size=(10, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (gru): GRU(200, 100, batch_first=True, dropout=0.5)\n",
              "  (linear): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDhMHsElAG0W"
      },
      "outputs": [],
      "source": [
        "def validation_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    validation_loss, correct, incorrect = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            validation_loss += loss_fn(pred, y).item()\n",
        "            correct += ((pred>=0.5) == y).type(torch.float).sum().item()\n",
        "            incorrect += ((pred>=0.5) != y).type(torch.float).sum().item()\n",
        "\n",
        "    validation_loss /= num_batches\n",
        "    correct /= size\n",
        "    incorrect /= size\n",
        "    print(f\"validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
        "    print(f\"Incorrect percentage : \\n Accuracy: {(100*incorrect):>0.1f}%\\n\")\n",
        "    return validation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSE3xBJOZzge"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a_NTvvjbVmE"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    validation_loss, correct, incorrect = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            validation_loss += loss_fn(pred, y).item()\n",
        "            correct += ((pred>=0.5) == y).type(torch.float).sum().item()\n",
        "            incorrect += ((pred>=0.5) != y).type(torch.float).sum().item()\n",
        "\n",
        "    validation_loss /= num_batches\n",
        "    correct /= size\n",
        "    incorrect /= size\n",
        "    print(f\"Test Stats: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
        "    print(f\"Incorrect percentage : \\n Accuracy: {(100*incorrect):>0.1f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTrainLoseAndError(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss, correct, incorrect = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            train_loss += loss_fn(pred, y).item()\n",
        "            correct += ((pred>=0.5) == y).type(torch.float).sum().item()\n",
        "            incorrect += ((pred>=0.5) != y).type(torch.float).sum().item()\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    correct /= size\n",
        "    incorrect /= size\n",
        "    print(f\"Train Error : \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
        "    print(f\"Incorrect percentage : \\n Accuracy: {(100*incorrect):>0.1f}%\\n\")"
      ],
      "metadata": {
        "id": "646fvqqSxMvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S8lj-1Wa41v"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.003\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(\"In the begining :\")\n",
        "getTrainLoseAndError(train_loader, model, loss_fn)\n",
        "validation_loop(validation_loader, model, loss_fn)\n",
        "print(\"------------------\")\n",
        "vLossMin = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_loader, model, loss_fn, optimizer)\n",
        "    vLoss = validation_loop(validation_loader, model, loss_fn)\n",
        "    if(vLossMin>vLoss):\n",
        "      torch.save(model, \"a2_lstm_fc2_do_gru.p\")\n",
        "      vLossMin = vLoss\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "print(\"In the End :\")\n",
        "getTrainLoseAndError(train_loader, model, loss_fn)\n",
        "validation_loop(validation_loader, model, loss_fn)\n",
        "print(\"------------------\")\n",
        "\n",
        "# learning_rate = 1e-3\n",
        "# batch_size = 64\n",
        "# epochs = 100\n",
        "# loss_fn = nn.BCELoss()\n",
        "# optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate) #try adagrad too\n",
        "\n",
        "# for t in range(epochs):\n",
        "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#     train_loop(train_loader, model, loss_fn, optimizer)\n",
        "#     test_loop(test_loader, model, loss_fn)\n",
        "# print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.load(\"a2_lstm_fc2_do_gru.p\").to(device)\n",
        "# getTrainLoseAndError(train_loader, model, loss_fn)\n",
        "# validation_loop(validation_loader, model, loss_fn)"
      ],
      "metadata": {
        "id": "A3MdQIupmC1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_glove_vector(glove_vec):\n",
        "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
        "    word_to_vec_map = {}\n",
        "    for line in f:\n",
        "      w_line = line.split()\n",
        "      curr_word = w_line[0]\n",
        "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
        "\n",
        "  return word_to_vec_map\n",
        "\n",
        "gloveFile = \"glove.6B.200d.txt\"\n",
        "word_to_vec_map = read_glove_vector(gloveFile)"
      ],
      "metadata": {
        "id": "pGnm8DI_cpIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_words = 300\n",
        "\n",
        "# def remove_tags(x):\n",
        "#     result = re.sub('<.*?>','',x)\n",
        "#     return result\n",
        "\n",
        "# def to_lo_vecs(sentence):\n",
        "#   lo_words =  sentence.strip().split()\n",
        "#   if len(lo_words)<max_words:\n",
        "#     lo_words = lo_words + [0]* (max_words-len(lo_words))\n",
        "#   else:\n",
        "#     lo_words = lo_words[:max_words]\n",
        "#   lo_vecs_glove = [word_to_vec_map['unk'] if x not in word_to_vec_map.keys() else word_to_vec_map[x] for x in lo_words]\n",
        "#   # lo_vecs_w2v = [w2vmodel['unk'] if x not in w2vmodel.wv.vocab else w2vmodel[x] for x in lo_words]\n",
        "#   # print(lo_vecs_glove.shape)\n",
        "#   # lo_vecs = np.append([lo_vecs_glove],[lo_vecs_w2v], axis = 1)\n",
        "#   return lo_vecs_glove #lo_vecs"
      ],
      "metadata": {
        "id": "AtRzioMycw5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenameTest = 'Test_Dataset.csv'"
      ],
      "metadata": {
        "id": "3E8H0FLQqdO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(filenameTest)\n",
        "print(len(df_test))\n",
        "\n",
        "# Preprocessing\n",
        "df_test.loc[:,\"review\"] = df_test.review.apply(lambda x : str.lower(x))\n",
        "df_test['review'] = df_test.review.apply(lambda x : remove_tags(x))\n",
        "df_test.loc[:,\"review\"] = df_test.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
        "### check if you want to remove stopwords\n",
        "df_test.loc[:,\"review\"] = df_test.review.apply(lambda sentence : to_lo_vecs(sentence))\n",
        "\n",
        "df_testp = df_test[df_test.sentiment == 'positive']\n",
        "df_testn = df_test[df_test.sentiment == 'negative'] # df.loc[df.sentiment == 'positive', 'sentiment'] = 1\n",
        "\n",
        "df_testp.drop('sentiment', inplace=True, axis=1)\n",
        "df_testn.drop('sentiment', inplace=True, axis=1)\n",
        "lenPosT = len(df_testp)\n",
        "print(lenPosT)\n",
        "lenNegT = len(df_testn)\n",
        "print(lenNegT)\n",
        "\n",
        "shape1 = (lenPosT,1)\n",
        "y1t = torch.ones(shape1)\n",
        "shape0 = (lenNegT,1)\n",
        "y0t = torch.zeros(shape0)\n",
        "yt = torch.cat((y1t,y0t)).to(device)\n",
        "print(yt.size())\n",
        "\n",
        "vecDataset = df_testp['review'].tolist()\n",
        "vecDatasetN = df_testn['review'].tolist()\n",
        "vecDataset.extend(vecDatasetN)\n",
        "del df_test\n",
        "del df_testn\n",
        "del df_testp\n",
        "x_data_test = torch.tensor(vecDataset, dtype=torch.float).to(device)\n",
        "testDataset = data_utils.TensorDataset(x_data_test, yt)\n",
        "test_loader = data_utils.DataLoader(testDataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArWr3ACPrIpK",
        "outputId": "dde2e241-491f-47c7-8873-e6991ab7fc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5110\n",
            "4878\n",
            "torch.Size([9988, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "model = torch.load(\"a2_lstm_fc2_do_gru.p\")\n",
        "test_loop(test_loader, model, loss_fn)"
      ],
      "metadata": {
        "id": "41dPkYxdrK5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52f53e2-85fd-491a-ef4b-e49eae167d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Stats: \n",
            " Accuracy: 51.2%, Avg loss: 0.693128 \n",
            "\n",
            "Incorrect percentage : \n",
            " Accuracy: 48.8%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JqdWmn-rd0xF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}